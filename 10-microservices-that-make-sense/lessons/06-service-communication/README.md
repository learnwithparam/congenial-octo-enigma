# 06 â€” Service Communication

In a monolith, components communicate through function calls. In a microservices architecture, components communicate over the network. This fundamental shift introduces latency, failure modes, and consistency challenges that do not exist in a single-process application. This lesson covers the two communication patterns in our LaunchPad project: synchronous HTTP calls between services and asynchronous messaging through Redis and BullMQ. We also examine eventual consistency and the role of idempotency in distributed systems.

## What You'll Learn

- How synchronous HTTP communication works between services (with practical examples using axios or fetch)
- How asynchronous communication works through Redis and BullMQ
- What eventual consistency means and why it is acceptable for many operations
- How to design idempotent operations that can be safely retried
- The trade-offs between synchronous and asynchronous patterns

## Key Concepts

### Synchronous HTTP Between Services

When one service needs an immediate answer from another, it makes a synchronous HTTP call. In our architecture, the most common example would be a service verifying a user's authentication token with the auth-service.

Consider a scenario where the notification-service needs to verify that the user sending a notification is authenticated. It would call the auth-service's verify endpoint:

```ts
// Example: notification-service verifying a token with auth-service
import axios from "axios";

const AUTH_SERVICE_URL = process.env.AUTH_SERVICE_URL || "http://localhost:3001";

async function verifyToken(token: string, correlationId: string) {
  try {
    const response = await axios.get(`${AUTH_SERVICE_URL}/auth/verify`, {
      headers: {
        Authorization: `Bearer ${token}`,
        "x-correlation-id": correlationId,
      },
      timeout: 3000, // 3 second timeout
    });
    return response.data; // { userId, email }
  } catch (err) {
    if (axios.isAxiosError(err) && err.response?.status === 401) {
      return null; // Token is invalid
    }
    throw err; // Network error or unexpected failure
  }
}
```

Several important practices are visible here.

The correlation ID is forwarded in the request headers. This is the same ID that was generated by the gateway and passed through x-correlation-id. By including it in service-to-service calls, every log entry across every service can be traced back to the original client request.

A timeout is configured. Without a timeout, a hung auth-service would cause the notification-service to hang too, consuming connections and threads until the entire system degrades. The 3-second timeout means the calling service fails fast and can return an error to its own caller.

Error handling distinguishes between expected failures (401 means the token is invalid, which is a normal case) and unexpected failures (network errors, 500s from the auth-service, which should propagate as errors).

### How the Gateway Proxies HTTP

In our project, the gateway handles the synchronous HTTP forwarding using http-proxy-middleware. The gateway does not make explicit HTTP calls like the axios example above. Instead, it proxies the raw HTTP connection:

```ts
app.use(
  route.path,
  createProxyMiddleware({
    target: route.target,
    changeOrigin: true,
    on: {
      proxyReq: (proxyReq, req) => {
        const corrId = (req as express.Request).correlationId;
        if (corrId) {
          proxyReq.setHeader("x-correlation-id", corrId);
        }
        proxyReq.setHeader("x-forwarded-by", "gateway");
      },
    },
  })
);
```

This is still synchronous communication. The gateway holds the client connection open until the downstream service responds. The difference from using axios directly is that the proxy passes through the raw HTTP stream, including headers, status codes, and body, without parsing or buffering the response in memory.

### Asynchronous Communication via Redis and BullMQ

When one service needs to trigger work in another service without waiting for it to complete, asynchronous messaging is the right choice. In our project, the notification-service is the primary consumer of async messages through BullMQ.

Consider a typical flow: after a user registers in the auth-service, you want to send a welcome email. The auth-service should not call the notification-service synchronously because:

- Sending an email is slow (500ms to several seconds)
- The registration response should not be delayed by email delivery
- If the email fails, the registration should still succeed

Instead, the auth-service publishes a message that the notification-service consumes:

```ts
// Example: auth-service publishing an event after registration
import { Queue } from "bullmq";

const notificationQueue = new Queue("notifications", {
  connection: { host: "localhost", port: 6379, maxRetriesPerRequest: null },
});

// Inside the register endpoint, after creating the user:
await notificationQueue.add("send-notification", {
  to: user.email,
  subject: "Welcome to LaunchPad",
  body: `Hi ${user.name}, your account has been created.`,
  type: "email",
  userId: user.id,
});
```

The auth-service adds a job to the queue and moves on. The notification-service worker picks it up asynchronously:

```ts
const worker = new Worker<NotificationPayload, unknown, string>(
  QUEUE_NAME,
  async (job) => {
    const { to, subject, body, type } = job.data;
    logger.info({ jobId: job.id, to, type, subject }, "Processing notification");
    await new Promise((resolve) => setTimeout(resolve, 500));
    return { delivered: true, timestamp: new Date().toISOString() };
  },
  { connection: { ...redisConnectionOpts }, concurrency: 5 }
);
```

The two services are completely decoupled in time. The auth-service does not know when or if the notification will be sent. It only knows that the job was successfully added to the queue in Redis.

### Eventual Consistency

In a monolith with a single database, operations are immediately consistent. When you register a user and send a welcome email in the same transaction, either both happen or neither does. In a microservices architecture, you sacrifice this guarantee in exchange for independence and resilience.

Eventual consistency means that after an event occurs, all parts of the system will eventually reflect that event, but not necessarily immediately. In our project:

1. The auth-service creates a user (immediately consistent within the auth service's own data)
2. A notification job is queued in Redis (this happens almost instantly, but the notification is not yet sent)
3. The BullMQ worker processes the job seconds or minutes later (the notification is eventually sent)

If the worker is down, the job stays in the queue. When the worker comes back, it processes the backlog. The system is eventually consistent: the user exists before the welcome email is sent, and there may be a delay, but the email will arrive.

This is acceptable because there is no business rule that says "a user does not exist until their welcome email is sent." The two operations are independent. The user can log in immediately after registration, regardless of whether the email has been delivered.

Where eventual consistency becomes problematic is when there is a genuine business dependency. If you need to confirm a payment before granting access, eventual consistency is not appropriate. You need synchronous communication (HTTP call to the payment service and wait for the response) or a saga pattern with compensating transactions.

### Idempotency

In a distributed system, messages can be delivered more than once. Network timeouts, retries, and queue processing can result in a single job being processed twice. Your services must handle this gracefully.

An idempotent operation produces the same result whether it is executed once or multiple times. BullMQ's retry mechanism is a perfect example of why this matters:

```ts
defaultJobOptions: {
  attempts: 3,
  backoff: {
    type: "exponential",
    delay: 1000,
  },
},
```

If the worker crashes while processing a notification (after sending the email but before reporting success to BullMQ), the job will be retried. Without idempotency, the user receives two identical welcome emails.

Strategies for idempotency:

Use unique identifiers. Each notification job has a unique ID. Before sending, check whether you have already sent a notification with this ID:

```ts
// Idempotent notification processing
async function processNotification(job) {
  const alreadySent = await checkDeliveryLog(job.id);
  if (alreadySent) {
    logger.info({ jobId: job.id }, "Already delivered, skipping");
    return { delivered: true, duplicate: true };
  }

  await sendEmail(job.data);
  await recordDelivery(job.id);
  return { delivered: true };
}
```

Use BullMQ's job ID deduplication. You can set a custom jobId when adding to the queue. BullMQ will reject duplicates:

```ts
await notificationQueue.add("send-notification", payload, {
  jobId: `welcome-email-${userId}`, // Same user = same job ID
});
```

The logout endpoint in our auth-service is naturally idempotent:

```ts
app.post("/auth/logout", (req, res) => {
  const authHeader = req.headers.authorization;
  if (authHeader?.startsWith("Bearer ")) {
    const token = authHeader.slice(7);
    sessions.delete(token);
  }
  res.json({ message: "Logged out" });
});
```

Calling logout twice with the same token succeeds both times. The first call deletes the session. The second call tries to delete a nonexistent session, which is a no-op. The response is always the same.

### Redis as the Communication Backbone

Redis serves multiple roles in our architecture:

- BullMQ queue storage: jobs are stored in Redis lists and sorted sets
- BullMQ job metadata: job state, progress, timestamps, and results live in Redis hashes
- Health check verification: services ping Redis to confirm connectivity
- Shared connection management: the shared/redis.ts module provides a singleton connection

The shared Redis module creates a connection with options that support BullMQ:

```ts
export function createRedisConnection(name: string): Redis {
  const redis = new Redis(REDIS_URL, {
    maxRetriesPerRequest: null, // Required by BullMQ
    lazyConnect: true,
  });

  redis.on("connect", () => {
    logger.info({ connection: name }, "Redis connected");
  });

  redis.on("error", (err) => {
    logger.error({ connection: name, err: err.message }, "Redis error");
  });

  return redis;
}
```

The lazyConnect option means the Redis connection is not established until the first command is sent. This prevents startup failures if Redis takes a moment to become available.

### Communication Pattern Decision Matrix

Here is a practical guide for choosing the right pattern:

Synchronous HTTP is appropriate for: user authentication (login, verify), data queries (get user profile, get notification status), and any operation where the caller cannot proceed without the result.

Asynchronous messaging is appropriate for: sending notifications, processing background jobs, triggering side effects (analytics, auditing), and any operation where the caller does not need the result immediately.

In our LaunchPad project:
- Client to Gateway: synchronous HTTP
- Gateway to Auth: synchronous HTTP (proxy)
- Gateway to Notifications: synchronous HTTP (proxy) for submitting and checking, but the actual notification delivery is async
- Auth to Notifications (hypothetical): async via BullMQ queue

## Step by Step

### Step 1: Identify Communication Needs

List every interaction between your services. For each one, determine whether the caller needs an immediate response.

### Step 2: Choose the Pattern

Use synchronous HTTP for immediate needs. Use async messaging for deferred work. When in doubt, start synchronous (it is simpler) and refactor to async if latency or reliability becomes an issue.

### Step 3: Set Timeouts on HTTP Calls

Every synchronous call between services must have a timeout. A reasonable default is 3-5 seconds for internal service calls.

### Step 4: Forward Correlation IDs

Every outbound request (HTTP or queue message) must include the correlation ID from the inbound request. This is non-negotiable for debugging in production.

### Step 5: Design for Retries

Make your operations idempotent. Use unique job IDs, delivery logs, or deduplication strategies so that processing a message twice does not cause incorrect behavior.

### Step 6: Handle Partial Failures

When a synchronous call fails, decide whether to fail the entire operation, return a degraded response, or queue a retry. There is no single right answer; it depends on the business requirement.

## Exercise

Design the communication patterns for a new checkout flow that spans three services:

1. The client calls POST /checkout through the gateway
2. The checkout service needs to verify the user's token with auth-service (sync or async?)
3. The checkout service needs to charge the user's payment method (sync or async?)
4. The checkout service needs to send a confirmation email (sync or async?)
5. The checkout service needs to update inventory counts (sync or async?)

For each interaction, justify your choice of sync vs async. Then implement a mock checkout service that demonstrates the pattern, using axios for synchronous calls and BullMQ for asynchronous messaging. Include proper timeout configuration, correlation ID forwarding, and idempotent job processing.

## Summary

Service communication is the defining challenge of microservices. Synchronous HTTP calls provide immediate responses but create temporal coupling between services. Asynchronous messaging through queues provides resilience and decoupling but introduces eventual consistency.

Our LaunchPad project uses both patterns: the gateway proxies synchronous HTTP to auth-service and notification-service, while the notification-service uses BullMQ for asynchronous job processing internally. Redis serves as the backbone for both patterns.

Every service-to-service call must include timeouts, correlation IDs, and error handling. Every operation that might be retried must be idempotent. These are not optional best practices; they are requirements for a reliable distributed system.

In the next lesson, we examine the shared-nothing architecture principle and why each service must own its own data.
